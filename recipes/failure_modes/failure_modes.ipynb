{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Failure Modes with AI Assisted Root Cause Analysis\n",
    "\n",
    "This recipe allows TensorZero users to analyze failure modes of their LLM application with help from Root Cause Analysis AI.\n",
    "Since TensorZero automatically logs all inferences and feedback, it is straightforward to analyze the failure modes of your LLM application on your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started:\n",
    "\n",
    "- Set the `TENSORZERO_CLICKHOUSE_URL` environment variable. For example: `TENSORZERO_CLICKHOUSE_URL=\"http://chuser:chpassword@localhost:8123/tensorzero\"`\n",
    "- Set the `OPENAI_API_KEY` environment variable.\n",
    "- Update the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"../../examples/data-extraction-ner/config/tensorzero.toml\"\n",
    "\n",
    "FUNCTION_NAME = \"extract_entities\"\n",
    "\n",
    "METRIC_NAME = \"jaccard_similarity\"\n",
    "\n",
    "# The name of the variant to use to grab the templates used for root cause analysis\n",
    "TEMPLATE_VARIANT_NAME = \"gpt_4o_mini\"\n",
    "\n",
    "# Optional list of tools available if your function supports them.\n",
    "# Each entry is formatted as as a dictionary.\n",
    "# {\"name\": \"<The tool's identifier.>\", \"description\": \"<A brief description of what the tool does.>\"}\n",
    "# These will be passed to the assistant to aid in root cause analysis.\n",
    "TOOLS_AVAILABLE = []\n",
    "\n",
    "# If the metric is a float metric, you can set the threshold to define a failure and filter the data\n",
    "FLOAT_METRIC_THRESHOLD = 0.5\n",
    "\n",
    "# Maximum number of samples to use for root cause analysis\n",
    "MAX_SAMPLES = 100_000\n",
    "\n",
    "# The name of the variant to use for root cause and failure mode analysis\n",
    "ANALYSIS_VARIANT_NAME = \"o4-mini\"\n",
    "\n",
    "# Embedding model to use for root cause and failure mode analysis\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "\n",
    "# Number of root cause clusters to use\n",
    "N_CLUSTERS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import toml\n",
    "from clickhouse_connect import get_client\n",
    "from minijinja import Environment\n",
    "from openai import AsyncOpenAI\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "from tensorzero import AsyncTensorZeroGateway\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "from utils import generate_root_causes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the TensorZero configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path(CONFIG_PATH)\n",
    "\n",
    "assert config_path.exists(), f\"{CONFIG_PATH} does not exist\"\n",
    "assert config_path.is_file(), f\"{CONFIG_PATH} is not a file\"\n",
    "\n",
    "with config_path.open(\"r\") as f:\n",
    "    config = toml.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the metric configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"metrics\" in config, \"No `[metrics]` section found in config\"\n",
    "assert METRIC_NAME in config[\"metrics\"], (\n",
    "    f\"No metric named `{METRIC_NAME}` found in config\"\n",
    ")\n",
    "\n",
    "metric = config[\"metrics\"][METRIC_NAME]\n",
    "\n",
    "metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the configuration for the variant with the templates we want to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"functions\" in config, \"No `[functions]` section found in config\"\n",
    "assert FUNCTION_NAME in config[\"functions\"], (\n",
    "    f\"No function named `{FUNCTION_NAME}` found in config\"\n",
    ")\n",
    "assert \"variants\" in config[\"functions\"][FUNCTION_NAME], (\n",
    "    f\"No variants section found for function `{FUNCTION_NAME}`\"\n",
    ")\n",
    "assert TEMPLATE_VARIANT_NAME in config[\"functions\"][FUNCTION_NAME][\"variants\"], (\n",
    "    f\"No variant named `{TEMPLATE_VARIANT_NAME}` found in function `{FUNCTION_NAME}`\"\n",
    ")\n",
    "\n",
    "function_type = config[\"functions\"][FUNCTION_NAME][\"type\"]\n",
    "variant = config[\"functions\"][FUNCTION_NAME][\"variants\"][TEMPLATE_VARIANT_NAME]\n",
    "\n",
    "variant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the system, user, and assistant templates in the variant (if any), and initialize a minijinja environment with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = {}\n",
    "\n",
    "if \"assistant_template\" in variant:\n",
    "    assistant_template_path = config_path.parent / variant[\"assistant_template\"]\n",
    "    with assistant_template_path.open(\"r\") as f:\n",
    "        templates[\"assistant\"] = f.read()\n",
    "\n",
    "if \"system_template\" in variant:\n",
    "    system_template_path = config_path.parent / variant[\"system_template\"]\n",
    "    with system_template_path.open(\"r\") as f:\n",
    "        system_template = f.read()\n",
    "        templates[\"system\"] = system_template\n",
    "else:\n",
    "    system_template = None\n",
    "\n",
    "if \"user_template\" in variant:\n",
    "    user_template_path = config_path.parent / variant[\"user_template\"]\n",
    "    with user_template_path.open(\"r\") as f:\n",
    "        templates[\"user\"] = f.read()\n",
    "\n",
    "env = Environment(templates=templates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the ClickHouse client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"TENSORZERO_CLICKHOUSE_URL\" in os.environ, (\n",
    "    \"TENSORZERO_CLICKHOUSE_URL environment variable not set\"\n",
    ")\n",
    "\n",
    "clickhouse_client = get_client(dsn=os.environ[\"TENSORZERO_CLICKHOUSE_URL\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the ClickHouse table name for the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_table_name = {\"chat\": \"ChatInference\", \"json\": \"JsonInference\"}.get(\n",
    "    function_type\n",
    ")\n",
    "\n",
    "if inference_table_name is None:\n",
    "    raise ValueError(f\"Unsupported function type: {function_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the ClickHouse table name for the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_table_name = {\n",
    "    \"float\": \"FloatMetricFeedback\",\n",
    "    \"boolean\": \"BooleanMetricFeedback\",\n",
    "}.get(metric[\"type\"])\n",
    "\n",
    "if feedback_table_name is None:\n",
    "    raise ValueError(f\"Unsupported metric type: {metric['type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the correct join key to use for the metric on the inference table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_join_key = {\n",
    "    \"episode\": \"episode_id\",\n",
    "    \"inference\": \"id\",\n",
    "}.get(metric[\"level\"])\n",
    "\n",
    "if inference_join_key is None:\n",
    "    raise ValueError(f\"Unsupported metric level: {metric['level']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query the inferences and feedback from ClickHouse.\n",
    "\n",
    "If the metric is a float metric, we need to filter the data for failures based on the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"optimize\" in metric, \"Metric is missing the `optimize` field\"\n",
    "\n",
    "threshold = FLOAT_METRIC_THRESHOLD if metric[\"type\"] == \"float\" else 0.5\n",
    "comparison_operator = \"<\" if metric[\"optimize\"] == \"max\" else \">\"\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    i.variant_name,\n",
    "    i.input,\n",
    "    i.output,\n",
    "    i.id,\n",
    "    f.value,\n",
    "    i.episode_id,\n",
    "FROM {inference_table_name} AS i\n",
    "JOIN (\n",
    "    SELECT\n",
    "        target_id,\n",
    "        value,\n",
    "        ROW_NUMBER() OVER (PARTITION BY target_id ORDER BY timestamp DESC) AS rn\n",
    "    FROM \n",
    "        {feedback_table_name}\n",
    "    WHERE\n",
    "        metric_name = %(metric_name)s\n",
    "        AND value {comparison_operator} %(threshold)s\n",
    ") f ON i.{inference_join_key} = f.target_id AND f.rn = 1\n",
    "\n",
    "WHERE \n",
    "    i.function_name  = %(function_name)s\n",
    "LIMIT %(max_samples)s\n",
    "\"\"\"\n",
    "\n",
    "params = {\n",
    "    \"function_name\": FUNCTION_NAME,\n",
    "    \"metric_name\": METRIC_NAME,\n",
    "    \"threshold\": threshold,\n",
    "    \"max_samples\": MAX_SAMPLES,\n",
    "}\n",
    "\n",
    "df = clickhouse_client.query_df(query, params)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Render the inputs using the templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_message(content: List[Dict[str, Any]], role: str, env: Environment) -> str:\n",
    "    assert role in [\"user\", \"assistant\"], f\"Invalid role: {role}\"\n",
    "    message = \"\"\n",
    "    delimeter = \"\\n\" if len(content) > 1 else \"\"\n",
    "    for c in content:\n",
    "        if c[\"type\"] == \"text\":\n",
    "            c = c[\"value\"]\n",
    "            if isinstance(c, str):\n",
    "                message += c + delimeter\n",
    "            else:\n",
    "                message += env.render_template(role, **c)  # type: ignore\n",
    "        elif c[\"type\"] == \"tool_call\":\n",
    "            message += f\"Tool call {c['name']}: {c['arguments']}{delimeter}\"\n",
    "        elif c[\"type\"] == \"tool_result\":\n",
    "            message += f\"Tool result {c['name']}: {c['result']}{delimeter}\"\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported content type: {c['type']}\")\n",
    "    return message\n",
    "\n",
    "\n",
    "def render_input(sample: Dict[str, Any]) -> Dict[str, List[Dict[str, Any]]]:\n",
    "    function_input = json.loads(sample[\"input\"])\n",
    "    rendered_input = {}\n",
    "\n",
    "    # Add the system message to the rendered messages\n",
    "    # If there is data passed in or a system template there must be a system message\n",
    "    system = function_input.get(\"system\", {})\n",
    "    if len(system) > 0 or system_template:\n",
    "        if system_template:\n",
    "            rendered_input[\"system\"] = env.render_template(\"system\", **system)\n",
    "        else:\n",
    "            rendered_input[\"system\"] = system\n",
    "\n",
    "    rendered_messages: List[Dict[str, Any]] = []\n",
    "    # Add the input messages to the rendered messages\n",
    "    for message in function_input[\"messages\"]:\n",
    "        rendered_message = render_message(message[\"content\"], message[\"role\"], env)\n",
    "        rendered_messages.append({\"role\": message[\"role\"], \"content\": rendered_message})\n",
    "\n",
    "    # Add the output to the messages\n",
    "    output = json.loads(sample[\"output\"])\n",
    "    if function_type == \"chat\":\n",
    "        if len(output) != 1:\n",
    "            raise ValueError(f\"Output {output} must have exactly one content block.\")\n",
    "\n",
    "        if output[0][\"type\"] == \"text\":\n",
    "            rendered_messages.append(\n",
    "                {\"role\": \"assistant\", \"content\": output[0][\"text\"]}\n",
    "            )\n",
    "        elif output[0][\"type\"] == \"tool_call\":\n",
    "            rendered_messages.append(\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": f\"Tool call {output[0]['name']}: {output[0]['arguments']}\",\n",
    "                }\n",
    "            )\n",
    "        elif output[0][\"type\"] == \"tool_result\":\n",
    "            rendered_messages.append(\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": f\"Tool result {output[0]['name']}: {output[0]['result']}\",\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported output type: {output[0]['type']}\")\n",
    "    elif function_type == \"json\":\n",
    "        rendered_messages.append({\"role\": \"assistant\", \"content\": output[\"raw\"]})\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported function type: {function_type}\")\n",
    "    rendered_input[\"messages\"] = rendered_messages\n",
    "    return rendered_input\n",
    "\n",
    "\n",
    "df[\"rendered_input\"] = df.apply(render_input, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root Cause Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a list of root causes for each failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gateway = await AsyncTensorZeroGateway.build_embedded(\n",
    "    config_file=\"config/tensorzero.toml\",\n",
    "    clickhouse_url=os.environ[\"TENSORZERO_CLICKHOUSE_URL\"],\n",
    ")\n",
    "semaphore = asyncio.Semaphore(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [\n",
    "    generate_root_causes(\n",
    "        gateway=gateway,\n",
    "        row=row.to_dict(),\n",
    "        variant_name=ANALYSIS_VARIANT_NAME,\n",
    "        metric_name=METRIC_NAME,\n",
    "        semaphore=semaphore,\n",
    "        tools_available=TOOLS_AVAILABLE,\n",
    "        dryrun=True,\n",
    "    )\n",
    "    for _, row in df.iterrows()\n",
    "]\n",
    "\n",
    "root_causes = await tqdm_asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_causes_concat = [\n",
    "    \"\\n\".join(root_cause) for root_cause in root_causes if root_cause is not None\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failure Mode Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1**: Get a vector representation of each root cause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = AsyncOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_embedding(text: str) -> Optional[list[float]]:\n",
    "    try:\n",
    "        async with semaphore:\n",
    "            response = await openai_client.embeddings.create(\n",
    "                input=text, model=EMBEDDING_MODEL\n",
    "            )\n",
    "            return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting embedding: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [get_embedding(root_cause) for root_cause in root_causes_concat]\n",
    "\n",
    "embeddings = await tqdm_asyncio.gather(*tasks, desc=\"Embedding inputs\")\n",
    "embeddings = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**: Find failure modes by clustering the root cause embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Bayesian GMM instead of KMeans\n",
    "bgmm = BayesianGaussianMixture(\n",
    "    n_components=N_CLUSTERS,\n",
    "    covariance_type=\"full\",\n",
    "    weight_concentration_prior_type=\"dirichlet_process\",\n",
    "    random_state=42,\n",
    ")\n",
    "bgmm.fit(embeddings)\n",
    "labels = bgmm.predict(embeddings)\n",
    "\n",
    "# Assign root cause labels to DataFrame\n",
    "df[\"root_cause\"] = root_causes_concat\n",
    "df[\"cluster\"] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure embeddings is a NumPy array\n",
    "embeddings = np.array(embeddings)\n",
    "\n",
    "# Fit PCA and transform\n",
    "pca = PCA(n_components=2)\n",
    "vis_dims2 = pca.fit_transform(embeddings)\n",
    "\n",
    "# Create DataFrame for plotting\n",
    "pca_df = pd.DataFrame(\n",
    "    {\"PC1\": vis_dims2[:, 0], \"PC2\": vis_dims2[:, 1], \"failure_mode\": df[\"cluster\"]}\n",
    ")\n",
    "\n",
    "# Compute cluster centroids\n",
    "centroids_df = pca_df.groupby(\"failure_mode\")[[\"PC1\", \"PC2\"]].mean().reset_index()\n",
    "centroids_df[\"label\"] = centroids_df[\"failure_mode\"].apply(lambda c: f\"Cluster {c}\")\n",
    "\n",
    "# Scatter plot of points\n",
    "points_chart = (\n",
    "    alt.Chart(pca_df)\n",
    "    .mark_circle(opacity=0.3, size=60)\n",
    "    .encode(\n",
    "        x=alt.X(\"PC1\", title=\"Principal Component 1\"),\n",
    "        y=alt.Y(\"PC2\", title=\"Principal Component 2\"),\n",
    "        color=alt.Color(\"failure_mode:N\", title=\"Failure Mode\"),\n",
    "        tooltip=[\"failure_mode\"],\n",
    "    )\n",
    ")\n",
    "\n",
    "# Cross markers for centroids\n",
    "centroids_chart = (\n",
    "    alt.Chart(centroids_df)\n",
    "    .mark_point(filled=True, size=100, shape=\"cross\")\n",
    "    .encode(x=\"PC1\", y=\"PC2\", color=alt.Color(\"failure_mode:N\"), tooltip=[\"label\"])\n",
    ")\n",
    "\n",
    "# Combine\n",
    "(points_chart + centroids_chart).properties(\n",
    "    title=\"Failure Modes visualized using Principal Component Analysis (PCA)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**: Summarize the failure modes in natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = []\n",
    "for i in range(N_CLUSTERS):\n",
    "    print(f\"Cluster {i}:\", end=\" \")\n",
    "\n",
    "    # Sample for summarization\n",
    "    root_causes_sample = df[df.cluster == i].root_cause.to_list()\n",
    "    # break\n",
    "\n",
    "    gateway_input = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"arguments\": {\n",
    "                            \"root_causes\": root_causes_sample,\n",
    "                            \"system_template\": system_template,\n",
    "                        },\n",
    "                    }\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    response = await gateway.inference(\n",
    "        input=gateway_input,\n",
    "        function_name=\"summarize_failure_modes\",\n",
    "        variant_name=ANALYSIS_VARIANT_NAME,\n",
    "    )\n",
    "    summary = response.output.parsed[\"summary\"]\n",
    "\n",
    "    # Sample for representative examples (different random seed to avoid duplication)\n",
    "    examples = df[df.cluster == i][\"rendered_input\"].to_list()\n",
    "    examples = [example[\"messages\"][1:] for example in examples]\n",
    "    summaries.append(summary)\n",
    "    pprint(f\"\\nSummary: {summary}\")\n",
    "\n",
    "    # Show example root causes\n",
    "    print(\"\\nRepresentative examples:\")\n",
    "    for ex in examples[:10]:\n",
    "        print(f\" - {ex}\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're all set!\n",
    "\n",
    "We encourage you to experiment with other parameters (e.g. `N_CLUSTERS`, `EMBEDDING_MODEL`, or the clustering algorithm).\n",
    "\n",
    "We use OpenAI o4-mini for the root cause and failure mode analysis.\n",
    "You can try using other models by adding variants to `config/tensorzero.toml` and updating `ANALYSIS_VARIANT_NAME`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
