{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsloth Supervised Fine-Tuning\n",
    "\n",
    "This recipe allows TensorZero users to fine-tune models using Unsloth and their own data.\n",
    "Since TensorZero automatically logs all inferences and feedback, it is straightforward to fine-tune a model using your own data and any prompt you want.\n",
    "\n",
    "To get started:\n",
    "\n",
    "- Set the `TENSORZERO_CLICKHOUSE_URL` environment variable. For example: `TENSORZERO_CLICKHOUSE_URL=\"http://chuser:chpassword@localhost:8123/tensorzero\"`\n",
    "- Update the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"../../../../examples/data-extraction-ner/config/tensorzero.toml\"\n",
    "\n",
    "FUNCTION_NAME = \"extract_entities\"\n",
    "\n",
    "# The name of the variant to use to grab the templates used for fine-tuning\n",
    "TEMPLATE_VARIANT_NAME = \"gpt_4o_mini\"  # It's OK that this variant uses a different model than the one we're fine-tuning\n",
    "\n",
    "# Fraction of the data to use for validation\n",
    "VAL_FRACTION = 0.2\n",
    "\n",
    "# Maximum number of samples to use for fine-tuning\n",
    "MAX_SAMPLES = 100_000\n",
    "\n",
    "# Maximum number of tokens per example to OpenAI\n",
    "MAX_TOKENS_PER_EXAMPLE = 65_536\n",
    "\n",
    "# The name of the model to fine-tune (supported models: https://platform.openai.com/docs/guides/fine-tuning)\n",
    "MODEL_NAME = \"gpt-4o-mini-2024-07-18\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext dotenv\n",
    "# %dotenv\n",
    "\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import numpy as numpy\n",
    "import toml\n",
    "from clickhouse_connect import get_client\n",
    "from minijinja import Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = Path(CONFIG_PATH)\n",
    "\n",
    "assert config_path.exists(), f\"{CONFIG_PATH} does not exist\"\n",
    "assert config_path.is_file(), f\"{CONFIG_PATH} is not a file\"\n",
    "\n",
    "with config_path.open(\"r\") as f:\n",
    "    config = toml.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"functions\" in config, \"No `[functions]` section found in config\"\n",
    "assert FUNCTION_NAME in config[\"functions\"], (\n",
    "    f\"No function named `{FUNCTION_NAME}` found in config\"\n",
    ")\n",
    "assert \"variants\" in config[\"functions\"][FUNCTION_NAME], (\n",
    "    f\"No variants section found for function `{FUNCTION_NAME}`\"\n",
    ")\n",
    "assert TEMPLATE_VARIANT_NAME in config[\"functions\"][FUNCTION_NAME][\"variants\"], (\n",
    "    f\"No variant named `{TEMPLATE_VARIANT_NAME}` found in function `{FUNCTION_NAME}`\"\n",
    ")\n",
    "\n",
    "function_type = config[\"functions\"][FUNCTION_NAME][\"type\"]\n",
    "variant = config[\"functions\"][FUNCTION_NAME][\"variants\"][TEMPLATE_VARIANT_NAME]\n",
    "\n",
    "variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = {}\n",
    "\n",
    "if \"assistant_template\" in variant:\n",
    "    assistant_template_path = config_path.parent / variant[\"assistant_template\"]\n",
    "    with assistant_template_path.open(\"r\") as f:\n",
    "        templates[\"assistant\"] = f.read()\n",
    "\n",
    "if \"system_template\" in variant:\n",
    "    system_template_path = config_path.parent / variant[\"system_template\"]\n",
    "    with system_template_path.open(\"r\") as f:\n",
    "        templates[\"system\"] = f.read()\n",
    "\n",
    "if \"user_template\" in variant:\n",
    "    user_template_path = config_path.parent / variant[\"user_template\"]\n",
    "    with user_template_path.open(\"r\") as f:\n",
    "        templates[\"user\"] = f.read()\n",
    "\n",
    "env = Environment(templates=templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"TENSORZERO_CLICKHOUSE_URL\" in os.environ, (\n",
    "    \"TENSORZERO_CLICKHOUSE_URL environment variable not set\"\n",
    ")\n",
    "\n",
    "clickhouse_client = get_client(dsn=os.environ[\"TENSORZERO_CLICKHOUSE_URL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_table_name = {\"chat\": \"ChatInference\", \"json\": \"JsonInference\"}.get(\n",
    "    function_type\n",
    ")\n",
    "\n",
    "if inference_table_name is None:\n",
    "    raise ValueError(f\"Unsupported function type: {function_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "SELECT\n",
    "    i.variant_name,\n",
    "    i.input,\n",
    "    i.output,\n",
    "    f.value,\n",
    "    i.episode_id\n",
    "FROM\n",
    "    {inference_table_name} i\n",
    "JOIN\n",
    "    (SELECT\n",
    "        inference_id,\n",
    "        value,\n",
    "        ROW_NUMBER() OVER (PARTITION BY inference_id ORDER BY timestamp DESC) as rn\n",
    "    FROM\n",
    "        DemonstrationFeedback\n",
    "    ) f ON i.id = f.inference_id AND f.rn = 1\n",
    "WHERE\n",
    "    i.function_name = %(function_name)s\n",
    "LIMIT %(max_samples)s\n",
    "\"\"\"\n",
    "\n",
    "params = {\n",
    "    \"function_name\": FUNCTION_NAME,\n",
    "    \"max_samples\": MAX_SAMPLES,\n",
    "}\n",
    "\n",
    "df = clickhouse_client.query_df(query, params)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_message(message: Dict[str, Any]) -> Optional[List[Dict[str, Any]]]:\n",
    "    role = message[\"role\"]\n",
    "    assert role in [\"user\", \"assistant\"], f\"Invalid role: {role}\"\n",
    "    content: List[Dict[str, Any]] = []\n",
    "    tool_calls: List[Dict[str, Any]] = []\n",
    "    rendered_messages: List[Dict[str, Any]] = []\n",
    "\n",
    "    for content_block in message[\"content\"]:\n",
    "        if content_block[\"type\"] == \"text\":\n",
    "            parsed_content = content_block[\"value\"]\n",
    "            if not isinstance(parsed_content, str):\n",
    "                parsed_content = env.render_template(role, **parsed_content)\n",
    "            content.append({\"type\": \"text\", \"text\": parsed_content})\n",
    "        elif content_block[\"type\"] == \"raw_text\":\n",
    "            content.append({\"type\": \"text\", \"text\": content_block[\"value\"]})\n",
    "        elif content_block[\"type\"] == \"thought\":\n",
    "            content.append(\n",
    "                {\"type\": \"text\", \"text\": f\"<think>{content_block['text']}</think>\"}\n",
    "            )\n",
    "        elif content_block[\"type\"] == \"tool_call\" and role == \"assistant\":\n",
    "            tool_calls.append(\n",
    "                {\n",
    "                    \"function\": {\n",
    "                        \"arguments\": json.dumps(content_block[\"arguments\"]),\n",
    "                        \"name\": content_block[\"name\"],\n",
    "                    },\n",
    "                    \"id\": content_block[\"id\"],\n",
    "                    \"type\": \"function\",\n",
    "                }\n",
    "            )\n",
    "        elif content_block[\"type\"] == \"tool_result\" and role == \"user\":\n",
    "            # Tool results get priority so that they follow the tool call in the conversation.\n",
    "            # Any other \"user\" content will be appended in another message below.\n",
    "            rendered_messages.append(\n",
    "                {\n",
    "                    \"role\": \"tool\",\n",
    "                    \"tool_call_id\": content_block[\"id\"],\n",
    "                    \"content\": content_block[\"result\"],\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                f\"We do not support content block type: {content_block['type']}, dropping example.\",\n",
    "                UserWarning,\n",
    "            )\n",
    "            return None\n",
    "\n",
    "    if content or tool_calls:\n",
    "        role_message: Dict[str, Any] = {\"role\": role}\n",
    "        if content:\n",
    "            role_message[\"content\"] = content\n",
    "        if tool_calls:\n",
    "            role_message[\"tool_calls\"] = tool_calls\n",
    "        rendered_messages.append(role_message)\n",
    "\n",
    "    return rendered_messages\n",
    "\n",
    "\n",
    "def render_output(\n",
    "    output: List[Dict[str, Any]],\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parses the assistant message from an observation using the provided function configuration.\n",
    "    \"\"\"\n",
    "    content: List[str] = []\n",
    "    tool_calls: List[Dict[str, Any]] = []\n",
    "\n",
    "    if function_type == \"json\":\n",
    "        return {\"role\": \"assistant\", \"content\": output[\"raw\"]}\n",
    "    elif function_type == \"chat\":\n",
    "        for content_block in output:\n",
    "            if content_block[\"type\"] == \"text\":\n",
    "                content.append({\"type\": \"text\", \"text\": content_block[\"text\"]})\n",
    "            elif content_block[\"type\"] == \"thought\":\n",
    "                content.append(\n",
    "                    {\"type\": \"text\", \"text\": f\"<think>{content_block['text']}</think>\"}\n",
    "                )\n",
    "            elif content_block[\"type\"] == \"tool_call\":\n",
    "                tool_calls.append(\n",
    "                    {\n",
    "                        \"function\": {\n",
    "                            \"arguments\": json.dumps(content_block[\"arguments\"]),\n",
    "                            \"name\": content_block[\"name\"],\n",
    "                        },\n",
    "                        \"id\": content_block[\"id\"],\n",
    "                        \"type\": \"function\",\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                warnings.warn(\n",
    "                    f\"We do not support content block type: {content_block['type']}, dropping example.\",\n",
    "                    UserWarning,\n",
    "                )\n",
    "                return None\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported function type: {function_type}\")\n",
    "\n",
    "    # Once we finish collecting all blocks, create one assistant message.\n",
    "    output_message: Dict[str, Any] = {\"role\": \"assistant\"}\n",
    "    if content:\n",
    "        output_message[\"content\"] = content\n",
    "    if tool_calls:\n",
    "        output_message[\"tool_calls\"] = tool_calls\n",
    "\n",
    "    return output_message\n",
    "\n",
    "\n",
    "def sample_to_openai_messages(sample) -> List[Dict[str, Any]]:\n",
    "    function_input = json.loads(sample[\"input\"])\n",
    "\n",
    "    rendered_messages = []\n",
    "\n",
    "    # Add the system message to the rendered messages\n",
    "    # If there is data passed in or a system template there must be a system message\n",
    "    system = function_input.get(\"system\", {})\n",
    "    if len(system) > 0 or system_template_path:\n",
    "        if system_template_path:\n",
    "            system_message = env.render_template(\"system\", **system)\n",
    "            rendered_messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "        else:\n",
    "            rendered_messages.append({\"role\": \"system\", \"content\": system})\n",
    "\n",
    "    # Add the input messages to the rendered messages\n",
    "    for message in function_input[\"messages\"]:\n",
    "        rendered_message = render_message(message)\n",
    "        if rendered_message is None:\n",
    "            # `render_message` will return None if the message contains an unknown or unsupported content block.\n",
    "            # The entire example is dropped if this is the case.\n",
    "            return None\n",
    "        rendered_messages.extend(render_message(message))\n",
    "\n",
    "    # Add the output to the messages\n",
    "    output = json.loads(sample[\"value\"])\n",
    "    rendered_output = render_output(output)\n",
    "    if rendered_output is None:\n",
    "        # `render_output` will return None if the output contains an unknown or unsupported content block.\n",
    "        # The entire example is dropped if this is the case.\n",
    "        return None\n",
    "    rendered_messages.append(rendered_output)\n",
    "\n",
    "    return {\"messages\": rendered_messages}\n",
    "\n",
    "\n",
    "df[\"openai_messages\"] = df.apply(sample_to_openai_messages, axis=1)\n",
    "\n",
    "# Drop null rows\n",
    "df = df[df[\"openai_messages\"].notna()]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_len = 2048\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    # torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    max_seq_len=max_seq_len,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
