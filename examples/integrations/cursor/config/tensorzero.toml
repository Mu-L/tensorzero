gateway.debug = true
# ┌────────────────────────────────────────────────────────────────────────────┐
# │                                 FUNCTIONS                                  │
# └────────────────────────────────────────────────────────────────────────────┘

[functions.cursorzero]
type = "chat"

[functions.cursorzero.variants."gpt-4.1"]
weight = 1
type = "chat_completion"
model = "openai::gpt-4.1"

[functions.cursorzero.variants."claude-3.7"]
weight = 1
type = "chat_completion"
model = "anthropic::claude-3-7-sonnet-20250219"

[functions.cursorzero.variants."gemini-2.5-pro"]
weight = 1
type = "chat_completion"
model = "google_ai_studio_gemini::gemini-2.5-pro-preview-03-25"

[functions.cursorzero.variants."o4-mini"]
weight = 1
type = "chat_completion"
model = "openai::o4-mini-2025-04-16"
extra_body = [{ pointer = "/temperature", value = 1.0 }]

[functions.cursorzero.variants.mercury_coder_small]
weight = 1
type = "chat_completion"
model = "mercury_coder_small"


# ┌────────────────────────────────────────────────────────────────────────────┐
# │                                  METRICS                                   │
# └────────────────────────────────────────────────────────────────────────────┘
[metrics.min_ted]
type = "float"
level = "inference"
optimize = "min"

[metrics.ted_ratio]
type = "float"
level = "inference"
optimize = "max"

# [metrics.valid_output]
# type = "boolean"
# level = "inference"
# optimize = "max"

[models.mercury_coder_small]
routing = ["inception"]

[models.mercury_coder_small.providers.inception]
type = "openai"
api_base = "https://api.inceptionlabs.ai/v1"
api_key_location = "env::INCEPTION_API_KEY"
model_name = "mercury-coder-small"
